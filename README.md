# ğŸ§  GoogLeNet Transfer Learning: CINIC-10 âœ CIFAR-10

This project implements a modular image classification pipeline using **GoogLeNet with Batch Normalization** in PyTorch. The model is first **pretrained on the CINIC-10 dataset**, then **fine-tuned on CIFAR-10**, with a **from-scratch training baseline** for comparison.

---

## ğŸ“Œ Features

- âœ… Modular training pipeline with CLI via `main.py`
- âœ… Pretraining on CINIC-10
- âœ… Fine-tuning on CIFAR-10 using pretrained weights
- âœ… Training from scratch on CIFAR-10
- âœ… Best model saving and early stopping
- âœ… TensorBoard logging support

---

## ğŸ“ Project Structure

```
GoogLeNet_CINIC10_CIFAR10/
â”‚
â”œâ”€â”€ main.py               # Unified entry point (train / finetune / scratch)
â”œâ”€â”€ train.py              # Pretraining on CINIC-10
â”œâ”€â”€ finetune.py           # Fine-tuning on CIFAR-10 using pretrained weights
â”œâ”€â”€ scratch.py            # Training from scratch on CIFAR-10
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ googlenet_bn.py   # GoogLeNet with BatchNorm
â”‚
â”œâ”€â”€ data_utils.py         # Data loading and augmentation
â”œâ”€â”€ configs.py            # Central configuration file
â”œâ”€â”€ utils.py              # Training loop, evaluation, logging, saving
â”‚
â”œâ”€â”€ checkpoints/          # Saved models (auto-created)
â”œâ”€â”€ runs/                 # TensorBoard logs
â”œâ”€â”€ CINIC-10/             # CINIC-10 dataset (downloaded and extracted)
â”œâ”€â”€ data/                 # CIFAR-10 download cache
â””â”€â”€ requirements.txt      # Python dependencies
```

---

## âš™ï¸ Setup

```bash
conda create -n googlenet_cinic10 python=3.9
conda activate googlenet_cinic10
pip install -r requirements.txt
```

---

## ğŸ“¥ Dataset Preparation

### CINIC-10
1. Download from: [CINIC-10 Official Download](https://datashare.ed.ac.uk/handle/10283/3192)  
2. Extract into the project root directory as `./CINIC-10/`  
   (Should contain `train/`, `valid/`, `test/` subfolders)

### CIFAR-10
No need to manually download. It will be automatically downloaded via `torchvision`.

---

## ğŸš€ How to Use

Run via the unified entry point `main.py` with one of three modes:

### 1. ğŸ§  Pretraining on CINIC-10
```bash
python main.py --mode train
```

- Uses: `train.py`
- Saves best model to: `checkpoints/googlenet_cinic10_best.pth`

---

### 2. ğŸ”§ Fine-tuning on CIFAR-10
```bash
python main.py --mode finetune
```

- Uses: `finetune.py`
- Loads pretrained weights from CINIC-10
- Trains only the final `fc` layer
- Saves best model to: `checkpoints/googlenet_finetuned_best.pth`

---

### 3. ğŸ§ª From-Scratch Training on CIFAR-10
```bash
python main.py --mode scratch
```

- Uses: `scratch.py`
- Trains all layers from random initialization
- Saves best model to: `checkpoints/googlenet_scratch_best.pth`

---

## ğŸ“ˆ TensorBoard Visualization

```bash
tensorboard --logdir runs
```

Then open [http://localhost:6006](http://localhost:6006) in your browser.

---

## ğŸ“Š Sample Results (for reference)

| Method          | Pretrained | Test Accuracy | Notes                  |
|-----------------|------------|---------------|------------------------|
| Pretrained      | CINIC-10   | ~74%          | Used as fine-tune base |
| Fine-tuned      | âœ… Yes      | **82%+**      | Fast convergence       |
| From Scratch    | âŒ No       | ~78% (Â±1%)    | Slower to converge     |

---

## ğŸ”® Future Work

- [ ] Add `test.py` for evaluation on CINIC-10 test set
- [ ] Add model inference on custom images
- [ ] Support for learning rate scheduling (e.g., cosine decay)
- [ ] Extend to CIFAR-100 or Tiny-ImageNet

---
